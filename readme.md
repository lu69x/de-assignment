# DE-assignment

Data Engineering assignment stack for practicing **Airflow orchestration**, **dbt transformation**, **MinIO storage**, and **Trino SQL query engine**.  
(‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ù‡∏∂‡∏Å Data Engineering: Orchestration, Transformation, Storage, Query)

---

## üèó Stack Overview
- **Apache Airflow** ‚Üí orchestrator (schedule & run DAGs)  
- **dbt** ‚Üí transformation (bronze ‚Üí silver ‚Üí gold)  
- **MinIO** ‚Üí S3-compatible object storage (data lake)  
- **Trino** ‚Üí SQL query engine (query data in MinIO)  
- **Docker Compose** ‚Üí run all services in containers  

---

## ‚úÖ Prerequisites
- Git  
- Docker ‚â• 20.x  
- Docker Compose ‚â• v2.x  
- Bash/Shell  
- RAM ‚â• 8 GB, CPU ‚â• 2‚Äì4 cores  
- Open ports:  
  - `8080` (Airflow)  
  - `8081/8082` (Trino)  
  - `9000/9001` (MinIO)  
- `.env` file with:
  - `AIRFLOW_UID`, `AIRFLOW_GID` (default: 50000)  
  - `MINIO_ROOT_USER`, `MINIO_ROOT_PASSWORD`  

> ‚ö†Ô∏è No Python needed on host ‚Äî everything runs inside containers.  

---

## ‚ö° Quick Start
1. Clone the repo  
   ```
   git clone https://github.com/lu69x/de-assignment.git
   cd de-assignment
   ```

2. Create .env file (if missing) and set required variables

3. Run initial setup
    ```
    sudo chmod 755 setup.sh
    ./setup.sh
    ```
    or
    ```
    bash setup.sh
    ```

4. Start all services
    ```
    docker compose -f full-build-docker-compose.yaml up -d
    ```
    To expose the interactive dbt docs console run the docs service as well:
    ```
    docker compose -f full-build-docker-compose.yaml up -d dbt-docs
    ```
5. Access services:
  - Airflow UI ‚Üí http://localhost:8080
  - MinIO Console ‚Üí http://localhost:9001
  - Trino Coordinator ‚Üí http://localhost:8081
  - dbt Docs Console ‚Üí http://localhost:8082 (or `http://localhost:${DBT_DOCS_PORT}` if customized)


## System Diagram
```mermaid
flowchart LR
  subgraph Docker_Network["Docker Network (docker-compose)"]
    A[Client / Analyst<br/>‚Ä¢ Airflow UI<br/>‚Ä¢ Trino SQL Client]:::ext

    subgraph Airflow_Stack["Apache Airflow"]
      A1[Webserver] --- A2[Scheduler] --- A3[Worker/Triggerer]
      A4[(Logs/Configs)]
    end

    subgraph Transform["dbt (models/seeds/macros)"]
      D1[dbt run / build]:::job
    end

    subgraph Storage["MinIO (S3-compatible)"]
      M1[[Bronze]]:::bronze
      M2[[Silver]]:::silver
      M3[[Gold]]:::gold
    end

    subgraph Query["Trino"]
      T1[Coordinator/Cluster]:::svc
      T2[(Catalogs & Connectors<br/>e.g., S3/MinIO, Hive, etc.)]:::meta
    end
  end

  %% Data flow
  A1 <-- manage DAGs / trigger --> A2
  A2 -->|run tasks| A3
  A3 -->|ingest raw files| M1
  A3 -->|invoke| D1
  D1 -->|read from| M1
  D1 -->|transform write| M2
  D1 -->|business marts| M3
  T1 -->|query tables| M2
  T1 -->|query marts| M3
  A:::hidden

  %% External access
  A --- A1
  A --- T1

classDef ext fill:#fff,stroke:#888,stroke-width:1px
classDef job fill:#eef,stroke:#557
classDef svc fill:#efe,stroke:#585
classDef meta fill:#fef,stroke:#a5a
classDef bronze fill:#f9e0,stroke:#d5a
classDef silver fill:#e6f2ff,stroke:#69f
classDef gold fill:#fff4b3,stroke:#cb3
classDef hidden display:none
```

## üìä Data Lineage (dbt Docs)
The `data_engineer_assignment` DAG now generates dbt documentation after every run. This produces the manifest, catalog, and the interactive lineage graph so you can trace every mart back to its staging sources.

1. When the DAG finishes the `publish_lineage_docs` task uploads the docs bundle to MinIO using the bucket/prefix configured by `S3_BUCKET` and `S3_DOCS_PREFIX` (default: `warehouse/assignment/docs`).
2. Each run is versioned by timestamp (e.g. `s3://warehouse/assignment/docs/20250101120000/index.html`) and a `latest/` alias is refreshed for convenience.
3. Open the MinIO Console ‚Üí bucket `warehouse` ‚Üí folder `assignment/docs/latest/` and download `index.html` (and the accompanying assets) or click the object preview to render the lineage graph directly in the browser.
4. You can also sync the folder locally with the MinIO CLI / AWS CLI: `aws --endpoint-url http://localhost:9000 s3 sync s3://warehouse/assignment/docs/latest ./docs` then open `./docs/index.html`.
5. ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏ä‡∏£‡πå‡∏ï‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ó‡∏µ‡∏°? ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏±‡∏ô‡πÄ‡∏ß‡πá‡∏ö‡πÄ‡∏ã‡∏¥‡∏£‡πå‡∏ü‡πÄ‡∏ß‡∏≠‡∏£‡πå‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢ ‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏™‡∏¥‡∏£‡πå‡∏ü‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÑ‡∏î‡πâ ‡πÄ‡∏ä‡πà‡∏ô `python -m http.server --directory ./docs 8000` ‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏´‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏£‡πà‡∏ß‡∏°‡∏ó‡∏µ‡∏°‡πÄ‡∏õ‡∏¥‡∏î `http://<‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì>:8000/index.html` ‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ô‡∏µ‡πâ‡πÑ‡∏õ‡∏¢‡∏±‡∏á static hosting ‡πÉ‡∏î ‡πÜ (S3, GitHub Pages, ‡∏Ø‡∏•‡∏Ø) ‡πÑ‡∏î‡πâ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ

### Live dbt docs console
- Start the containerized console with `docker compose up -d dbt-docs`. The service automatically runs `dbt deps`, regenerates docs, and launches `dbt docs serve` on port `8082` (override via `DBT_DOCS_PORT`).
- After the DAG publishes a fresh bundle, refresh the browser at `http://localhost:8082` to explore the interactive lineage graph without downloading files manually.
- The console mounts your local `dbt/` folder, so edits to models/macros can be reloaded by restarting the service.

### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ï‡πà‡∏≠‡∏¢‡∏≠‡∏î‡∏´‡∏•‡∏±‡∏á‡πÑ‡∏î‡πâ docs
- **‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå lineage**: ‡πÄ‡∏õ‡∏¥‡∏î `index.html` ‡πÅ‡∏•‡πâ‡∏ß‡∏™‡∏≥‡∏£‡∏ß‡∏à lineage graph ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏ß‡πà‡∏≤‡∏ï‡∏≤‡∏£‡∏≤‡∏á downstream ‡∏û‡∏∂‡πà‡∏á‡∏û‡∏≤ source ‡πÉ‡∏î‡∏ö‡πâ‡∏≤‡∏á ‡∏ä‡πà‡∏ß‡∏¢‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏Å‡πà‡∏≠‡∏ô‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÇ‡∏°‡πÄ‡∏î‡∏•
- **‡πÅ‡∏ô‡∏ö‡πÉ‡∏ô PR / RFC**: ‡∏ñ‡πà‡∏≤‡∏¢ screenshot ‡∏´‡∏£‡∏∑‡∏≠ export diagram ‡∏à‡∏≤‡∏Å dbt docs ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏õ‡∏∞‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏ó‡∏µ‡∏°‡πÄ‡∏´‡πá‡∏ô‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á pipeline
- **‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô living documentation**: ‡∏ï‡∏±‡πâ‡∏á automation ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° (‡πÄ‡∏ä‡πà‡∏ô Airflow sensor ‡∏´‡∏£‡∏∑‡∏≠ GitHub Action) ‡πÉ‡∏´‡πâ sync `assignment/docs/latest/` ‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏ä‡∏£‡πå‡∏Ç‡∏≠‡∏á‡∏ó‡∏µ‡∏°‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà DAG ‡∏£‡∏±‡∏ô‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á lineage ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡πÑ‡∏î‡πâ‡πÄ‡∏™‡∏°‡∏≠
- **‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**: ‡∏™‡πà‡∏ß‡∏ô `Tests` ‡πÉ‡∏ô dbt docs ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô test ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏ó‡∏µ‡∏° Data/Analytics ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á

> Tip: dbt exposures defined under `dbt/models/schema.yml` describe downstream dashboards so the lineage view highlights how marts power each consumer.
